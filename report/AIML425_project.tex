%  Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx,hyperref}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{AIML 425 Project}
%
% Single address.
% ---------------
\name{Quan Zhao (Student ID: 300471028)}
%\name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
\address{Victoria University of Wellington}


\begin{document}
%\ninept
%
\maketitle
%
\section{Introduction}
\label{sec:intro}

Face Detection is a very popular research topic and have various applications in industry.
Nerual Network plays key role in this work, lots of Face Detection Nerual Networks has been introduced.
RetinaFace is one of most populars implementation, which is been introduced in 2019, 
and be accepted by "Conference on Computer Vision and Pattern Recognition (CVPR)" in 2020.
In this study, I will show my understanding of this work based on this paper "RetinaFace: Single-shot Multi-level Face Localisation in the Wild" \cite{deng2020retinaface}
In real world, lots of edge case can not be covered by this pretrained model.
Retrain whole model cost too much.
Fine tuning model approach will be introduced in this study.


\section{Content Analysis}
\label{sec:content}

In paper "RetinaFace: Single-shot Multi-level Face Localisation in the Wild" \cite{deng2020retinaface}, 
Author introduced an approach which is a  novel pixel-wise face localization method based on a single-stage design call RetinaFace. 
The method employs a multi-task learning strategy to simultaneously predict various attributes related to a face. 

\subsection{RetinaFace}

RetinaFace, as the name suggests, is a specialized adaptation of the RetinaNet architecture tailored for face detection. While it retains the core principles of RetinaNet, it introduces additional components specifically designed for face detection, such as:

\begin{enumerate}
\item Predicting facial landmarks in addition to face bounding boxes.
\item Incorporating a mesh decoder to predict pixel-wise 3D face shapes.
\end{enumerate}

In this study, we will focus more on the face detection capabilities of RetinaFace. We aim to delve deeper into its workings and explore how to fine-tune it.

\subsection{RetinaNet}

RetinaNet \cite{lin2017focal} is a single-stage object detector that introduced the Focal Loss to address the class imbalance problem inherent in object detection tasks. The name "RetinaNet" is derived from its use of a feature pyramid network (FPN) to detect objects at different scales, similar to the multi-scale processing in the human retina.

The architecture and features of RetinaNet are:

\begin{enumerate}
  \item Single-stage vs. Two-stage Detectors:
  \begin{itemize}
    \item Before RetinaNet, there were primarily two categories of object detectors: single-stage detectors (like YOLO and SSD) that are fast but less accurate, and two-stage detectors (like Faster R-CNN) that are more accurate but slower.

    \item RetinaNet was designed as a single-stage detector but aimed to achieve the accuracy levels of two-stage detectors.
  \end{itemize}
  \item Feature Pyramid Network (FPN):

  \begin{itemize}
    \item RetinaNet incorporates the Feature Pyramid Network (FPN) to efficiently detect objects at different scales.
    \item FPN constructs a multi-scale pyramid of feature maps by combining low-resolution, semantically strong features with high-resolution, semantically weak features using a top-down pathway and lateral connections.
    \item This multi-scale representation is crucial for detecting objects of varying sizes in images.
  \end{itemize}
  \item Focal Loss:
  \begin{itemize}
  \item One of the main challenges with single-stage detectors is the class imbalance between foreground (object) and background classes. Since these detectors densely sample possible object locations in an image, there are many more negative samples (background) than positive samples (objects).
  \item RetinaNet introduced the Focal Loss to address this imbalance. The Focal Loss is designed to down-weight the contribution of easy negatives (background samples that are easily classified as background) and focus more on the hard negatives and positives.
  \item This dynamic scaling of the loss helps in training the model more effectively on challenging samples.
\end{itemize}
  \item Anchor Boxes:
  \begin{itemize}
    \item Like many object detectors, RetinaNet uses anchor boxes (pre-defined bounding boxes) at each spatial location in its feature maps to predict object locations.
    \item Multiple anchor boxes with different scales and aspect ratios are used at each location to cater to objects of different shapes and sizes.
\end{itemize}
  \item Classification and Regression Heads:
  \begin{itemize}
    \item For each anchor box, RetinaNet has a classification head that determines the object class (or background) and a regression head that refines the bounding box coordinates.
    \item These heads are shared across all the scales in the FPN, ensuring consistent predictions across different object sizes.
\end{itemize}
  \item Backbone Network:

  \begin{itemize}
    \item RetinaNet typically uses a deep convolutional network, like ResNet, as its backbone to extract feature maps from the input image. These feature maps then feed into the FPN.
  \end{itemize}
\end{enumerate}

\subsubsection{deeper into Focal Loss}

The Focal Loss is a specialized loss function introduced to address the class imbalance problem in object detection, particularly for single-stage detectors.

% Motivation
In object detection, especially with single-stage detectors, there's a significant class imbalance between the background class (no object) and the object classes. This is because detectors densely sample possible object locations in an image, leading to many more negative samples (background) than positive samples (objects).

Traditional cross-entropy loss treats every sample equally, which means the vast number of easy negative samples can dominate the loss and lead to degenerate models.

% Formulation
The Focal Loss is designed to down-weight the contribution of easy samples and focus more on the hard samples.
It is defined as:
\[ \text{FL}(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t) \]
where:

\begin{itemize}
    \item \( p_t \) is the probability of the true class.
    \item \( \alpha_t \) is a balancing factor.
    \item \( \gamma \) is the focusing parameter that adjusts the rate at which easy samples are down-weighted.
    \item When \( \gamma = 0 \), Focal Loss is the same as cross-entropy loss. As \( \gamma \) increases, the effect of the modulating factor becomes more pronounced.
\end{itemize}


\begin{enumerate}
  \item Effects of the Focusing Parameter \( \gamma \):
  \begin{itemize}
    \item The modulating factor \( (1 - p_t)^\gamma \) ensures that easy examples (those with a high value of \( p_t \) for the true class) have a reduced loss.
    \item As \( \gamma \) increases, the effect of this modulation intensifies, meaning that the loss for easy examples decreases rapidly.
    \item This allows the model to focus more on hard, misclassified examples.
\end{itemize}
  \item Balancing Factor \( \alpha_t \):
  \begin{itemize}
    \item \( \alpha_t \) can be set to balance the importance of positive/negative classes. For instance, it can be set inversely proportional to the class frequencies.
    \item It's an additional factor to handle extreme class imbalance.
\end{itemize}
  \item Benefits:
  \begin{itemize}
    \item By using the Focal Loss, models are less prone to being overwhelmed by abundant easy negative samples.
    \item It allows single-stage detectors, like RetinaNet, to achieve state-of-the-art performance by addressing one of the main challenges they face.
\end{itemize}
\end{enumerate}

% Conclusion:

The Focal Loss is a pivotal innovation in the realm of object detection. By dynamically scaling the loss based on the true class probability, it ensures that the training process is not dominated by easy-to-classify samples. This focus on challenging samples leads to improved model performance, especially in scenarios with significant class imbalances.

\subsection{feature pyramid network (FPN)}

In traditional computer vision, pyramids were used to process images at multiple scales to detect objects of varying sizes.
Deep convolutional neural networks (CNNs) inherently produce feature maps at different resolutions, with shallow layers capturing high-resolution but semantically weak features and deep layers capturing low-resolution but semantically strong features.
FPN \cite{lin2017feature} aims to leverage these multi-scale features effectively for object detection.

It consists of the following parts.

\begin{enumerate}
  \item Bottom-up Pathway:
  
    This is the standard forward pass in a CNN. As you go deeper into the network, the spatial resolution of the feature maps decreases, but the semantic information increases.

  \item Top-down Pathway and Lateral Connections:
  
    - FPN introduces a top-down pathway where the spatial resolution is upsampled.

    - During this upsampling, FPN also adds lateral connections from the bottom-up pathway. These lateral connections are feature maps from the same level in the bottom-up pathway, which are merged with the upsampled features.
    
    - This merging process combines high-level semantic information from the top-down pathway with detailed spatial information from the bottom-up pathway.
  
  \item Multi-scale Predictions:
    
    With the combined features at different levels of the pyramid, FPN can make predictions at multiple scales. This allows for the detection of objects of varying sizes with high accuracy.

  \item Benefits:
  
    - Improved Detection Across Scales: Traditional CNNs often struggled with detecting small objects because the feature maps in deeper layers, which have strong semantic information, are of low resolution. FPN addresses this by combining low-resolution, semantically strong features with high-resolution, semantically weak features.

    - Efficiency: Instead of processing the image at multiple scales separately, FPN efficiently constructs a feature pyramid from a single-scale image, reducing computational overhead.

  \item Usage in Modern Architectures:
  
    FPN has become a foundational component in many state-of-the-art object detection architectures, including RetinaNet. By providing multi-scale feature representations, it enhances the detector's ability to recognize objects of various sizes.

\end{enumerate}

Feature Pyramid Network (FPN) is a pivotal advancement in object detection, addressing the challenge of detecting objects across different scales. By integrating multi-scale feature representations in a unified architecture, FPN enhances both the accuracy and efficiency of object detectors.


\subsection{Anchor based Face Deteciton on pyramid network}

In RetinaFace, the anchors are indeed densely tiled across the feature maps of each pyramid level, such as P2. However, the anchors don't predict for every single pixel in the sense of moving pixel by pixel. Instead, they are associated with the spatial locations on the feature map, and their density is determined by the stride of that particular pyramid level.

Let's break this down:

\begin{enumerate}
  \item Stride: Each pyramid level has an associated stride, which indicates the spatial resolution difference between the original image and the feature map at that level. For instance, if P2 has a stride of 4, it means each spatial location in the P2 feature map corresponds to a 4x4 region in the original image.
  \item Anchor Tiling: For each spatial location on a feature map (e.g., P2), multiple anchors of different scales and aspect ratios are tiled. So, if you're at a specific x,y location on P2, there will be multiple anchors centered at that location, each with a different size or shape.
  \item Dense Sampling: While the anchors are densely sampled, they aren't associated with every pixel of the original image. Instead, they are associated with the spatial locations of the feature map. Using the P2 example with a stride of 4: for the first spatial location (top-left), the anchors might be centered at pixel (2,2) of the original image (assuming the anchor's center is at the middle of the 4x4 region). The next set of anchors on P2 would be centered at pixel (6,2), and so on.
  \item Coverage: Even though the anchors aren't predicting for every single pixel, the combination of multiple scales and aspect ratios ensures that they can potentially cover all objects (faces, in the case of RetinaFace) in the image. The dense tiling, combined with the varying sizes and shapes of the anchors, allows the model to detect faces of different sizes and orientations.
\end{enumerate}
In summary, while RetinaFace does densely tile anchors across the feature maps of each pyramid level, it doesn't predict for every pixel in a pixel-by-pixel manner. Instead, the predictions are made for spatial locations on the feature map, with the density determined by the stride of the pyramid level.

\subsection{Context Module in RetinaFace}

The "context module" is a significant component of the RetinaFace architecture.

The context module in RetinaFace is designed to capture contextual information from different feature levels. In many face detection architectures, multi-level feature fusion is a common strategy to handle faces of various sizes and in different scenarios. The context module in RetinaFace aims to enhance the feature representation by aggregating context from different scales.

\begin{enumerate}
  \item Multi-level Feature Aggregation: The context module takes features from different levels of the network. These features come from various resolutions, capturing both high-level and low-level details.
  \item Feature Enhancement: By aggregating these multi-level features, the context module can enhance the representation of each feature level. This is particularly useful for detecting small faces or faces in challenging scenarios where contextual information can provide additional clues.
  \item Improved Detection Performance: The inclusion of the context module in RetinaFace helps in achieving better detection performance, especially in scenarios where faces are partially occluded, are of varying sizes, or are in challenging lighting conditions.
\end{enumerate}

In essence, the context module serves as a mechanism to integrate and leverage information from different scales, enhancing the robustness and accuracy of the RetinaFace detector in diverse real-world scenarios.

\subsubsection{How Context Module works with FPN}

\begin{enumerate}
  \item Feature Aggregation: The context module takes the multi-scale features provided by the FPN and further refines them by aggregating context. This can be thought of as a two-step enhancement: first, the FPN provides features at multiple scales, and then the context module refines these features by considering broader contextual information.
  \item Improved Detection: By combining the strengths of FPN and the context module, RetinaFace achieves a richer feature representation. This enhanced representation is more robust and versatile, allowing the model to detect faces with higher accuracy, especially in challenging scenarios.
\end{enumerate}

In summary, while the FPN in RetinaFace provides a multi-scale feature representation, the context module further refines these features by aggregating contextual information. The combination of these two components results in a powerful and robust face detection mechanism.


\subsection{Cascade Regression}

Cascade regression refers to a series of regression operations performed in a cascaded manner. In the context of object detection (or face detection, in the case of RetinaFace), this often means refining the bounding box predictions in a step-by-step manner. Instead of predicting the final bounding box coordinates in one go, the model makes an initial prediction and then refines it in subsequent steps.

The idea behind cascade regression is that iterative refinement can lead to more accurate and stable bounding box predictions, especially in challenging scenarios.

\subsection{Multi-task Loss}

The multi-task loss in the RetinaFace paper is designed to handle multiple objectives simultaneously, including face classification, bounding box regression, facial landmark regression, and dense regression.

\[ L = L_{cls}(p_i, p^*i) + \lambda_1 1{p^i} L*{box}(t_i, t^*i) \]
\[+ \lambda_2 1{p^i} L*{pts}(l_i, l^*i) + \lambda_3 1{p^i} L*{pixel} \]

Where:

\begin{itemize}
\item \( 1_{p^*_i} \) is an indicator function that is 1 for positive anchors and 0 otherwise.
\item \( \lambda_1, \lambda_2, \lambda_3 \) are loss-balancing parameters.
\end{itemize}

In the RetinaFace paper, the loss-balancing parameters \( \lambda_1, \lambda_2, \lambda_3 \) are set to 0.25, 0.1, and 0.01, respectively. This means that the model places a higher significance on better box and landmark locations derived from supervision signals.

Now, breakdown this formla.

\begin{enumerate}
  \item Face Classification Loss (\(L_{cls}\)):
  \begin{itemize}
    \item This loss is responsible for determining whether a given anchor is a face or not.
    \item \( p_i \) is the predicted probability of anchor \( i \) being a face.
    \item \( p^*_i \) is the ground truth, where it's 1 for a positive anchor (face) and 0 for a negative anchor (non-face).
    \item The classification loss \( L_{cls} \) uses the softmax loss for binary classes (face/not face).
  \end{itemize}
    \item Face Box Regression Loss (\(L_{box}\)):
    \begin{itemize}
      \item This loss refines the bounding box coordinates for detected faces.
      \item \( t_i = \{t_{x}, t_{y}, t_{w}, t_{h}\} \) represents the coordinates of the predicted box.
      \item \( t^*_i = \{t^{x}, t^{y}, t^*_{w}, t^*_{h}\} \) represents the ground truth box coordinates associated with a positive anchor.
      \item The regression targets (i.e., center location, width, and height) are normalized, and the loss \( L_{box} \) uses the smooth-L1 loss as defined in the Fast R-CNN paper.
    \end{itemize}
    \item Facial Landmark Regression Loss (\(L_{pts}\)):
    \begin{itemize}
      \item This loss predicts the locations of five facial landmarks.
      \item \( l_i = \{l_{x1}, l_{y1}, ..., l_{x5}, l_{y5}\} \) represents the predicted five facial landmarks.
      \item \( l^*_i = \{l^{x1}, l^{y1}, ..., l^*_{x5}, l^*_{y5}\} \) represents the ground truth landmarks associated with a positive anchor.
    \item Similar to the box center regression, the facial landmark regression also employs target normalization based on the anchor center.
    \end{itemize}
    \item Dense Regression Loss (\(L_{pixel}\)):
    \begin{itemize}
      \item This loss is related to the dense regression branch of the model, which is designed to provide dense facial landmarks.
      \item The specifics of this loss are further detailed in the paper, especially in relation to the mesh decoder.
    \end{itemize}
  \end{enumerate}

\subsection{Cascade Regression with Multi-task Loss}

\begin{enumerate}
\item Iterative Bounding Box Refinement: Within the RetinaFace framework, cascade regression is applied to the bounding box regression task. The model first predicts an initial bounding box. This prediction is then refined in subsequent stages, each time getting closer to the ground truth.
\item Joint Optimization: While the bounding box predictions are being refined through cascade regression, the model still jointly optimizes the other tasks (face classification and landmark localization) using the multi-task loss. This ensures that all tasks benefit from the iterative refinement process.
\item Improved Accuracy: The combination of cascade regression and multi-task loss means that the model can achieve more accurate bounding box predictions without compromising the performance of the other tasks. The iterative refinement process allows the model to correct any initial inaccuracies in the bounding box predictions, leading to better overall detection performance.
\end{enumerate}

In essence, by combining cascade regression with a multi-task loss, RetinaFace ensures that the bounding box predictions are iteratively refined, leading to more accurate and stable face detections. This iterative refinement process works in tandem with the other tasks, ensuring that the overall performance of the model is optimized.

\section{Impact}

The "RetinaFace: Single-shot Multi-level Face Localisation in the Wild" paper has significantly influenced the domain of face detection. Its contributions have paved the way for a plethora of subsequent research endeavors. This section delves into the multifaceted impact of RetinaFace on the research community.

\subsection{Improving the Architecture}
The foundational architecture of RetinaFace has not only been influential in human face detection but has also been adapted for other species. A remarkable adaptation is presented by \cite{xu2022cattlefacenet}, titled "CattleFaceNet: A cattle face identification approach based on RetinaFace and ArcFace loss." This research innovatively leverages the strengths of RetinaFace for cattle face identification, integrating it with the ArcFace loss to enhance recognition accuracy. Such adaptations underscore the versatility of the RetinaFace model and its potential to serve as a base for diverse identification tasks.

\subsection{Adapting to Different Domains}
The adaptability of RetinaFace is showcased by its application in various unique scenarios beyond traditional face detection. A significant example is the study by \cite{aswal2020single}, titled "Single Camera Masked Face Identification." This research emphasizes the challenges and nuances of identifying faces when obscured by masks, a scenario that has become increasingly relevant in recent times. It demonstrates the flexibility of the RetinaFace model to cater to domain-specific challenges and its potential to address contemporary issues.

\subsection{Optimization for Real-time Applications}
The need for real-time face detection, especially in resource-constrained environments, has driven researchers to optimize RetinaFace for speed and efficiency. A notable contribution in this domain is the work by \cite{putro2022faster}, titled "A Faster Real-time Face Detector Support Smart Digital Advertising on Low-cost Computing Device." This research underscores the importance of efficient face detection in the realm of digital advertising, particularly on low-cost computing devices, further emphasizing the adaptability and potential of the RetinaFace model for real-world applications.

\section{Fine tuning pretrained model}

Fine-tuning a pretrained face detection model involves adapting a model that has been trained on a large dataset to a new, typically smaller, dataset. 
This can be useful when we have a specific dataset or use-case in mind that might differ from the original training data. 

There are three potential approach can be used to fine-tuning the pretrained model,
which are fine-tuning hypter-parameters, fine-tuning by freeze early layers and fine-tuning with different anchors.

\subsection{Fine-tuning hypter-parameters}

RetinaFace is a prominent facial detection model pretrained on a diverse dataset. Even though it exhibits commendable performance right out of the box, for specific applications or datasets, there might be a need to adjust its hyper-parameters to achieve optimal performance. This section discusses the key hyper-parameters and emphasizes those that need particular attention during the fine-tuning phase.

Among the several hyper-parameters that 

RetinaFace uses, the following are crucial for performance optimization:

\begin{enumerate}
    \item \textbf{NMS Threshold (nms\_threshold)}: 
    
    This parameter represents the Intersection over Union (IoU) threshold. When the IoU of two bounding boxes exceeds this threshold, the bounding box with the lower confidence score is suppressed during the Non-Maximum Suppression (NMS) process. The default value for this threshold is set to 0.4, but it lies within the range [0.0, 1.0]. For some applications, adjusting this threshold can help reduce false positives or improve the recall.
    
    \item \textbf{Visualization Threshold (vis\_thres)}: 
    
    This is the confidence threshold applied after the NMS process and before the final output. It filters detections based on their confidence scores, allowing only those with scores above this threshold to be part of the final output. Its default value is 0.6, and it can be adjusted between [0.0, 1.0]. Fine-tuning this parameter can help in reducing noise in the final detections.
\end{enumerate}

The other parameters such as \textit{confidence\_threshold}, \textit{top\_k}, and \textit{keep\_top\_k} are set to their default values of 0.02, 5000, and 750 respectively. 

Their settings generally ensure that the model is sufficiently sensitive (but not overly so) and that a large enough number of top detections are kept both before and after the NMS process.

Fine-tuning these hyper-parameters, 

especially the NMS and visualization thresholds, plays a pivotal role in the model's performance. 

This is because the balance between precision and recall in object detection tasks is sensitive to these values. A lower NMS threshold might result in more bounding boxes being suppressed, leading to fewer false positives but potentially missing some true positives. 

On the other hand, adjusting the visualization threshold can control the quality of detections in the final output. Therefore, by fine-tuning these parameters to suit a specific dataset or application, one can achieve a more desirable balance between precision and recall, ensuring that the model's detections are both accurate and comprehensive.

\subsection{Fine-tuning with Frozen Early Layers}

Fine-tuning pretrained models on novel datasets is a prevalent strategy in deep learning, offering a balance between leveraging pre-existing knowledge and adapting to new data. A common approach during this process is to freeze the early layers of the model, allowing only the deeper layers to be updated. This section delves into the rationale behind this approach and its implications for model performance.

\subsubsection{Rationale for Freezing Early Layers}

\begin{enumerate}
\item Transfer of Generic Features: Deep neural networks, particularly convolutional architectures, have a hierarchical feature extraction process. The initial layers often capture universal, low-level features such as edges, textures, and colors. These foundational features are generally applicable across diverse datasets, making them valuable for transfer learning.

\item Mitigation of Overfitting: Fine-tuning all layers with a small novel dataset can lead to overfitting, where the model becomes overly specialized to the training data. By freezing the early layers, the number of trainable parameters is reduced, thus decreasing the model's capacity to overfit.

\item Computational Efficiency: Training fewer parameters can expedite the convergence of the model, leading to faster training epochs and potentially requiring fewer epochs to achieve optimal performance on the new dataset.
\end{enumerate}

\subsubsection{Why Freezing Early Layers Works?}
\begin{enumerate}
  \item Hierarchical Feature Learning: 
  
  As data progresses through the layers of a neural network, features transition from being generic to specific. 

Early layers often act as filters detecting fundamental patterns, while deeper layers combine these patterns to recognize more complex structures. 
When transferring to a new task, the basic patterns remain largely consistent, while the higher-level representations require adjustments to cater to the specifics of the new data.

\item Regularization Effect: 

By not allowing early layers to change, the model is implicitly regularized. This constraint can be viewed as a form of structural regularization, where certain parts of the model are kept constant, guiding the learning process in the deeper layers and preventing them from fitting to noise in the new dataset.

\item Data-driven Adaptation: 

While the early layers remain fixed, the deeper layers, which are more adaptable and data-specific, undergo training. This ensures that the model retains its generalization capability from the pretraining phase while fine-tuning based on the nuances of the new dataset.
\end{enumerate}

Freezing the early layers during fine-tuning strikes a balance between leveraging pre-existing, generic features and adapting to novel data characteristics. This approach, rooted in the hierarchical nature of feature extraction in deep networks, offers a computationally efficient and effective method for transfer learning across diverse tasks. Future research might explore adaptive freezing strategies, where the decision to freeze or train layers is data-driven, further optimizing the fine-tuning process.


\subsection{Fine-tuning with different anchors}

Face detection models often rely on predefined bounding boxes, known as anchors, to predict the location of faces in an image. The accuracy of these models can be significantly influenced by the choice of these anchors, especially when dealing with diverse face sizes and orientations. In this section, we delve into the methodology and impact of fine-tuning our model with different anchor configurations.

\subsubsection{Anchor Analysis and Selection}

Before embarking on the fine-tuning process, it's paramount to understand the distribution of face sizes and aspect ratios in the dataset. This preliminary analysis provides insights into potential anchor sizes and ratios that might be most effective.

To determine the optimal sizes for our anchors, we employed k-means clustering on the ground truth bounding boxes. The centroids of these clusters provided a representative set of widths and heights for our anchors. While the number of anchors is often set between 5 and 9 in many applications, 
our dataset's unique characteristics led us to choose \textit{TODO:X} anchors (where \textit{TODO:X} is the number you decided on).

\subsubsection{Anchor Ratios and Scaling}

Beyond mere size, the aspect ratio of anchors plays a pivotal role in accurately capturing the variability of faces in images. Common ratios, such as 1:1, 1:2, and 2:1, were considered based on the dataset's distribution.

Due to FPN has been used in RetinaFace models that utilize multiple feature maps to detect objects at varying scales.
following the implementation in RetinaFace we use same number anchors for feature maps.

\subsubsection{Training and Results}

With the newly determined anchors, the model was trained and its performance closely monitored. 

Preliminary results indicated a \textit{TODO:Y}\% improvement in accuracy (where \textit{TODO:Y} is the improvement percentage you observed). However, it's worth noting that while anchor fine-tuning can enhance performance, it's just one facet of the optimization process. Factors like data augmentation, model architecture, and loss functions also significantly influence the model's efficacy.

\subsubsection{Conclusion}

Fine-tuning with different anchors offers a promising avenue for enhancing face detection models. By tailoring the anchors to the specific characteristics of the dataset, we can achieve more accurate and robust face detections. Future work might explore the iterative refinement of anchors and delve deeper into the interplay between anchor configurations and other model hyperparameters.


\section{RESULTS}
\label{sec:results}

\subsection{Data Preparation}
\label{ssec:data}


  \subsection{Model Fine Tuning and evaluation}
  \label{ssec:model}

\section{Impact}
  
\section{CONCLUSION}
\label{sec:conclusion}


\section{STATEMENT OF ALL TOOLS USED}
\label{sec:statementofalltoolsused}

% In this work, we used Pytorch geometric package to generate data, create, train models. 
% The Plotly package helped us visualize in 3D. 

Source codes are published in github: 
% $\href{}{URL}$




% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak

\vfill\pagebreak

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{strings,refs}

\end{document}